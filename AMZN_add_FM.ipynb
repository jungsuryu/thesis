{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action = 'ignore')\n",
    "\n",
    "import pickle\n",
    "import itertools\n",
    "\n",
    "import matplotlib.pyplot as plot\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### review embedding through sentence embedding (SBERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-05 15:03:24.094272: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-05 15:03:24.276839: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-05 15:03:24.338729: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-05 15:03:25.109254: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-12-05 15:03:25.109358: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-12-05 15:03:25.109372: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 불러오기\n",
    "ratings = pd.read_csv('/home/ryu/thesis/data/amazon/Amazon_ratings.csv')\n",
    "reviews = pd.read_csv('/home/ryu/thesis/data/amazon/Amazon_reviews.csv')\n",
    "\n",
    "cnt = ratings.groupby('user_id').count()['rating']\n",
    "keys = cnt[cnt>3].keys()\n",
    "ratings = ratings[ratings['user_id'].isin(keys)]\n",
    "\n",
    "ratings = ratings[['item_id', 'user_id', 'rating']]\n",
    "reviews = reviews[['item_id', 'user_id', 'text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A07936821FOVJO6NP4Q8</td>\n",
       "      <td>B0000A0AEM</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Very nice product.  Sharp, clear optics.  Seem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A07936821FOVJO6NP4Q8</td>\n",
       "      <td>B0000AI0N1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Just what it said it would be, well made, wort...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A07936821FOVJO6NP4Q8</td>\n",
       "      <td>B0002RSPE4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Bought a bunch of these a few years ago for th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A07936821FOVJO6NP4Q8</td>\n",
       "      <td>B000CRFOMK</td>\n",
       "      <td>5.0</td>\n",
       "      <td>At first glance these seem silly.  Who needs a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A07936821FOVJO6NP4Q8</td>\n",
       "      <td>B000ID7QNI</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Very sturdy well made wall mount.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241856</th>\n",
       "      <td>AZZTOUKVTUMVM</td>\n",
       "      <td>B003FVJYF8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>It's 50' of coax. Seems to be well constructed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241857</th>\n",
       "      <td>AZZTOUKVTUMVM</td>\n",
       "      <td>B00BUL4NLU</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Bought this at the end of September 2016 for u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241858</th>\n",
       "      <td>AZZTOUKVTUMVM</td>\n",
       "      <td>B00HWT8I24</td>\n",
       "      <td>3.0</td>\n",
       "      <td>It was a bit challenging to get this to synch ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241859</th>\n",
       "      <td>AZZTOUKVTUMVM</td>\n",
       "      <td>B00IX9ZDKC</td>\n",
       "      <td>5.0</td>\n",
       "      <td>What can I say. It worked great and my compute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241860</th>\n",
       "      <td>AZZTOUKVTUMVM</td>\n",
       "      <td>B00TQD2BYK</td>\n",
       "      <td>4.0</td>\n",
       "      <td>It was a bit temperamental to get it set-up, b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>241861 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     item_id     user_id  rating  \\\n",
       "0       A07936821FOVJO6NP4Q8  B0000A0AEM     3.0   \n",
       "1       A07936821FOVJO6NP4Q8  B0000AI0N1     5.0   \n",
       "2       A07936821FOVJO6NP4Q8  B0002RSPE4     5.0   \n",
       "3       A07936821FOVJO6NP4Q8  B000CRFOMK     5.0   \n",
       "4       A07936821FOVJO6NP4Q8  B000ID7QNI     5.0   \n",
       "...                      ...         ...     ...   \n",
       "241856         AZZTOUKVTUMVM  B003FVJYF8     4.0   \n",
       "241857         AZZTOUKVTUMVM  B00BUL4NLU     2.0   \n",
       "241858         AZZTOUKVTUMVM  B00HWT8I24     3.0   \n",
       "241859         AZZTOUKVTUMVM  B00IX9ZDKC     5.0   \n",
       "241860         AZZTOUKVTUMVM  B00TQD2BYK     4.0   \n",
       "\n",
       "                                                     text  \n",
       "0       Very nice product.  Sharp, clear optics.  Seem...  \n",
       "1       Just what it said it would be, well made, wort...  \n",
       "2       Bought a bunch of these a few years ago for th...  \n",
       "3       At first glance these seem silly.  Who needs a...  \n",
       "4                       Very sturdy well made wall mount.  \n",
       "...                                                   ...  \n",
       "241856  It's 50' of coax. Seems to be well constructed...  \n",
       "241857  Bought this at the end of September 2016 for u...  \n",
       "241858  It was a bit challenging to get this to synch ...  \n",
       "241859  What can I say. It worked great and my compute...  \n",
       "241860  It was a bit temperamental to get it set-up, b...  \n",
       "\n",
       "[241861 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.merge(ratings, reviews, how='left', left_on=['user_id', 'item_id'], right_on=['user_id', 'item_id'])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data['text'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = list(data['text'].values)\n",
    "model.max_seq_length = 10\n",
    "\n",
    "embeddings = model.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.24346013e-02 -9.56571754e-03  2.12417319e-02 -2.76257265e-02\n",
      " -5.12140989e-02 -7.77860358e-02  1.22302666e-01  5.86405359e-02\n",
      " -6.17323034e-02  2.80550290e-02  4.90620397e-02  2.57509574e-02\n",
      "  9.32610035e-03  8.58614687e-03 -7.86167979e-02  2.91688144e-02\n",
      "  2.83283349e-02 -7.43849576e-02  1.46041084e-02  7.81145005e-04\n",
      "  9.40033048e-03  7.16467574e-03  4.32437751e-03  9.40016285e-03\n",
      " -3.05967480e-02  1.17141008e-02  5.56921251e-02  1.54577019e-02\n",
      "  4.19114232e-02 -5.58091588e-02 -1.76646039e-02  1.18749365e-02\n",
      " -3.09459548e-02  8.87921639e-03  2.07308419e-02 -4.78523374e-02\n",
      "  2.12100428e-02 -4.11934219e-02  7.81660806e-03  2.13949438e-02\n",
      " -1.23685608e-02 -2.33170353e-02 -3.11087933e-04 -3.00242915e-03\n",
      " -5.80964126e-02  5.50509570e-03  7.41629824e-02  2.67745964e-02\n",
      "  3.72200459e-02 -6.32630140e-02 -6.80268332e-02 -3.77624221e-02\n",
      " -7.09628016e-02 -9.74218249e-02  3.39796208e-02  7.00396895e-02\n",
      " -8.04503113e-02 -1.65358055e-02  2.18628477e-02 -7.02033713e-02\n",
      "  4.44306657e-02 -2.46475935e-02 -1.09448470e-01  2.01309081e-02\n",
      "  7.39033818e-02  1.71161443e-02 -2.42489446e-02 -1.25495777e-01\n",
      "  2.08649430e-02 -4.39494327e-02  2.46760249e-03  4.39941995e-02\n",
      "  2.83457479e-03  3.97518789e-03 -6.82152435e-02 -5.17736115e-02\n",
      "  2.88925078e-02 -3.10852043e-02 -1.35725662e-02  5.10692634e-02\n",
      " -6.43375097e-03 -7.34554902e-02 -4.05088626e-02  3.29365022e-02\n",
      "  1.89240351e-02  1.23207318e-02 -4.92983218e-03 -3.26830782e-02\n",
      " -4.11483794e-02 -4.88092527e-02  3.29128094e-02  8.07982609e-02\n",
      " -1.05519556e-01 -2.11221483e-02 -5.10021597e-02 -1.47974677e-02\n",
      "  4.95451540e-02 -4.18248661e-02 -1.51688103e-02  1.18921712e-01\n",
      "  5.68393581e-02 -2.82176863e-02 -4.88692941e-03 -4.09969836e-02\n",
      " -7.72432461e-02  1.45509271e-02  4.08799015e-02  8.51291269e-02\n",
      " -3.47420350e-02  9.78395063e-03 -3.39979418e-02  4.88235764e-02\n",
      " -1.37354121e-01 -2.68681832e-02 -2.24008765e-02  4.63112742e-02\n",
      " -8.51675589e-03  3.66775356e-02  9.15737376e-02 -1.10647388e-01\n",
      "  3.84150222e-02 -1.19747845e-02 -2.60562617e-02  1.00286700e-01\n",
      " -4.24724147e-02  2.67010219e-02  1.57027673e-02 -3.26916954e-33\n",
      "  5.09246392e-03  1.35544717e-01 -3.20788026e-02 -5.59378462e-03\n",
      " -2.31359471e-02  5.82201313e-03 -1.22478092e-02  6.80115148e-02\n",
      " -2.78819799e-02  1.86199676e-02  9.15692188e-03  8.72743055e-02\n",
      " -1.68987922e-02  7.30058327e-02  2.84598600e-02 -1.14443600e-01\n",
      "  4.13866043e-02  6.23953044e-02 -7.20746964e-02  4.66957651e-02\n",
      " -8.98572803e-02  7.06618279e-02 -7.23466603e-03  3.60604562e-02\n",
      "  1.51567729e-02 -3.57522368e-02 -3.32087232e-03  6.11019433e-02\n",
      "  2.37332247e-02  4.27623242e-02  5.57325855e-02  1.17826171e-01\n",
      "  5.50403856e-02 -4.14956659e-02 -6.84654340e-02  2.37532463e-02\n",
      " -1.09181449e-01 -3.33418325e-02  9.05284006e-03  5.51067591e-02\n",
      " -2.01510359e-02  9.21403021e-02  9.49768629e-03  8.39417242e-03\n",
      "  5.95518015e-02  4.55414020e-02 -9.06313509e-02  7.71445259e-02\n",
      " -8.08648951e-03  1.31527288e-02 -1.24525772e-02 -2.72854399e-02\n",
      " -3.42646278e-02 -1.48397470e-02  8.13814811e-03  2.68671084e-02\n",
      " -2.40106471e-02 -4.03481349e-02 -4.01313370e-03 -6.54491708e-02\n",
      "  3.33673432e-02 -2.71719601e-02 -6.10482618e-02  2.06229575e-02\n",
      " -8.12926590e-02 -2.44392524e-03 -2.09117308e-02  5.47703169e-02\n",
      "  1.90858375e-02 -1.23470156e-02 -8.30511078e-02  3.77126560e-02\n",
      "  4.39981185e-02 -4.55631651e-02  3.35748866e-02 -4.57343459e-03\n",
      " -5.39327860e-02 -5.00207767e-03  6.26812726e-02  4.56472822e-02\n",
      " -5.31104915e-02  4.96758632e-02 -3.28161591e-03 -1.13807850e-01\n",
      " -9.10462886e-02  2.55777650e-02 -1.63966939e-02 -7.33920485e-02\n",
      " -4.31366675e-02 -2.71242484e-02 -1.12479851e-02 -2.95665977e-03\n",
      " -1.91523489e-02  3.10424110e-03 -1.01471961e-01  1.55221623e-33\n",
      "  2.53260341e-02 -7.06938654e-03  2.08026245e-02  1.19815215e-01\n",
      " -1.00564491e-02  2.64314115e-02  4.86256108e-02  6.38241693e-02\n",
      " -1.55916400e-02  5.40971495e-02  9.47826505e-02  3.01596764e-02\n",
      " -3.05746123e-02 -2.09866129e-02 -3.79458331e-02 -1.34179825e-02\n",
      "  9.74811390e-02 -2.91516650e-02 -6.66582771e-03 -1.03930376e-01\n",
      "  5.91453537e-02  2.37878524e-02  2.70076152e-02 -5.64545430e-02\n",
      " -8.25855043e-03  2.50903461e-02 -6.06109016e-02 -6.04389049e-02\n",
      " -6.54416904e-02  1.28200799e-02 -4.59983572e-03 -4.21506502e-02\n",
      "  2.41524074e-02 -2.08783895e-02 -2.44887769e-02  5.77910319e-02\n",
      "  8.74322057e-02 -7.92596564e-02 -1.96289644e-02 -5.58277108e-02\n",
      "  3.92141193e-02  1.51128629e-02  6.28542453e-02  1.70778371e-02\n",
      " -3.43362316e-02 -4.47730161e-02  1.00331686e-01  2.47095414e-02\n",
      " -5.22034764e-02 -9.77568328e-03 -3.07371281e-03  7.73034291e-03\n",
      "  2.97877807e-02 -3.42310071e-02 -5.49881905e-02 -7.53346682e-02\n",
      " -6.31873775e-03  3.40241566e-02  3.52710299e-02  4.98921648e-02\n",
      " -7.76925590e-03  2.89142225e-02 -9.80852917e-02  2.41044629e-02\n",
      "  6.66737333e-02 -2.63458658e-02  7.79858902e-02  4.60314155e-02\n",
      "  1.37700830e-02  4.15836833e-02  7.15120435e-02 -1.37052061e-02\n",
      " -6.44651428e-03  2.27185376e-02  7.58184940e-02 -7.91878998e-02\n",
      "  1.01060458e-01 -7.76253315e-03 -2.75716074e-02  1.02998525e-01\n",
      "  3.69557110e-03 -2.17654835e-02 -4.53937463e-02  5.72190620e-02\n",
      "  1.90263074e-02 -4.93516885e-02 -2.00986303e-02 -7.78074637e-02\n",
      " -5.02779745e-02  8.32104608e-02 -3.20876092e-02  4.78413776e-02\n",
      " -2.99628228e-02 -4.84410338e-02  1.76932767e-03 -1.75625861e-08\n",
      "  8.50456282e-02  4.62224670e-02 -4.21027690e-02 -1.82286985e-02\n",
      " -2.57216711e-02 -9.95316431e-02  2.85534421e-03  1.31333293e-03\n",
      " -2.11556107e-02 -9.17044953e-02 -4.40379558e-03 -5.16282488e-03\n",
      " -9.99419764e-02  7.77896568e-02  6.22989722e-02 -2.88978145e-02\n",
      " -5.18000498e-02  1.04364738e-01 -2.55665276e-02 -1.11374855e-02\n",
      " -1.15236430e-03  1.62726119e-02  1.00994259e-01  9.76194721e-03\n",
      " -7.12443367e-02  4.77085412e-02 -5.85668581e-03  2.17379257e-02\n",
      "  4.84016947e-02  7.02712610e-02  1.15707773e-03  7.01617077e-02\n",
      "  9.00764018e-02 -1.04313456e-02 -4.17006388e-02  2.71616522e-02\n",
      " -6.64585307e-02  2.99279280e-02 -1.36321113e-02  5.26877195e-02\n",
      " -4.94220369e-02 -6.73622563e-02 -9.27400403e-03  1.03504723e-02\n",
      "  7.35838935e-02 -1.72720160e-02  7.45046586e-02 -2.23168824e-03\n",
      " -9.02815834e-02  3.89436483e-02  1.34391170e-02  5.38041629e-02\n",
      " -4.16347245e-03 -2.02705599e-02 -9.79033411e-02 -4.56162170e-02\n",
      "  8.95698443e-02 -4.33814265e-02 -4.58329283e-02  4.85340459e-03\n",
      "  4.44589667e-02  3.08124479e-02 -5.08003458e-02  1.16876833e-01]\n"
     ]
    }
   ],
   "source": [
    "print(embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(241861, 384)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩 저장\n",
    "with open('/home/ryu/thesis/new_amazon/sbert_emb.pickle', 'wb') as f:\n",
    "    pickle.dump(embeddings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/ryu/thesis/new_amazon/sbert_emb.pickle', 'rb') as f:\n",
    "    embeddings = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>374</th>\n",
       "      <th>375</th>\n",
       "      <th>376</th>\n",
       "      <th>377</th>\n",
       "      <th>378</th>\n",
       "      <th>379</th>\n",
       "      <th>380</th>\n",
       "      <th>381</th>\n",
       "      <th>382</th>\n",
       "      <th>383</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.032435</td>\n",
       "      <td>-0.009566</td>\n",
       "      <td>0.021242</td>\n",
       "      <td>-0.027626</td>\n",
       "      <td>-0.051214</td>\n",
       "      <td>-0.077786</td>\n",
       "      <td>0.122303</td>\n",
       "      <td>0.058641</td>\n",
       "      <td>-0.061732</td>\n",
       "      <td>0.028055</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.097903</td>\n",
       "      <td>-0.045616</td>\n",
       "      <td>0.089570</td>\n",
       "      <td>-0.043381</td>\n",
       "      <td>-0.045833</td>\n",
       "      <td>0.004853</td>\n",
       "      <td>0.044459</td>\n",
       "      <td>0.030812</td>\n",
       "      <td>-0.050800</td>\n",
       "      <td>0.116877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.046252</td>\n",
       "      <td>0.053345</td>\n",
       "      <td>0.023569</td>\n",
       "      <td>-0.005799</td>\n",
       "      <td>-0.001641</td>\n",
       "      <td>0.005328</td>\n",
       "      <td>0.016217</td>\n",
       "      <td>-0.065180</td>\n",
       "      <td>-0.021635</td>\n",
       "      <td>0.044293</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040859</td>\n",
       "      <td>0.082676</td>\n",
       "      <td>0.012692</td>\n",
       "      <td>0.078683</td>\n",
       "      <td>-0.095079</td>\n",
       "      <td>0.007839</td>\n",
       "      <td>0.124836</td>\n",
       "      <td>-0.024412</td>\n",
       "      <td>-0.000559</td>\n",
       "      <td>0.004015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.058179</td>\n",
       "      <td>-0.028349</td>\n",
       "      <td>0.028041</td>\n",
       "      <td>-0.001780</td>\n",
       "      <td>-0.056059</td>\n",
       "      <td>-0.040762</td>\n",
       "      <td>0.035624</td>\n",
       "      <td>0.013983</td>\n",
       "      <td>-0.043915</td>\n",
       "      <td>-0.016364</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.044998</td>\n",
       "      <td>-0.046685</td>\n",
       "      <td>0.003407</td>\n",
       "      <td>0.010634</td>\n",
       "      <td>0.013347</td>\n",
       "      <td>-0.002498</td>\n",
       "      <td>-0.083653</td>\n",
       "      <td>-0.098793</td>\n",
       "      <td>-0.052496</td>\n",
       "      <td>0.050222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001419</td>\n",
       "      <td>-0.005161</td>\n",
       "      <td>0.001925</td>\n",
       "      <td>-0.017756</td>\n",
       "      <td>0.003331</td>\n",
       "      <td>-0.007262</td>\n",
       "      <td>0.074666</td>\n",
       "      <td>0.010081</td>\n",
       "      <td>0.028236</td>\n",
       "      <td>0.082903</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016537</td>\n",
       "      <td>-0.042396</td>\n",
       "      <td>-0.030921</td>\n",
       "      <td>0.020760</td>\n",
       "      <td>-0.070070</td>\n",
       "      <td>-0.002282</td>\n",
       "      <td>0.037053</td>\n",
       "      <td>0.009862</td>\n",
       "      <td>0.059242</td>\n",
       "      <td>0.050374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.044808</td>\n",
       "      <td>0.021749</td>\n",
       "      <td>-0.026192</td>\n",
       "      <td>-0.037803</td>\n",
       "      <td>-0.068915</td>\n",
       "      <td>-0.040948</td>\n",
       "      <td>-0.054282</td>\n",
       "      <td>0.100183</td>\n",
       "      <td>-0.018990</td>\n",
       "      <td>0.021407</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.094901</td>\n",
       "      <td>-0.043734</td>\n",
       "      <td>0.030118</td>\n",
       "      <td>0.018265</td>\n",
       "      <td>0.024000</td>\n",
       "      <td>-0.043978</td>\n",
       "      <td>0.041587</td>\n",
       "      <td>0.014160</td>\n",
       "      <td>0.012837</td>\n",
       "      <td>0.041781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241856</th>\n",
       "      <td>-0.028359</td>\n",
       "      <td>0.060098</td>\n",
       "      <td>0.007660</td>\n",
       "      <td>-0.044228</td>\n",
       "      <td>-0.021465</td>\n",
       "      <td>-0.005880</td>\n",
       "      <td>-0.066491</td>\n",
       "      <td>0.068297</td>\n",
       "      <td>-0.028384</td>\n",
       "      <td>0.022716</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036754</td>\n",
       "      <td>-0.022991</td>\n",
       "      <td>0.036425</td>\n",
       "      <td>-0.062824</td>\n",
       "      <td>-0.098093</td>\n",
       "      <td>-0.005447</td>\n",
       "      <td>-0.070600</td>\n",
       "      <td>0.031582</td>\n",
       "      <td>0.085118</td>\n",
       "      <td>-0.016995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241857</th>\n",
       "      <td>0.001685</td>\n",
       "      <td>0.009930</td>\n",
       "      <td>0.017188</td>\n",
       "      <td>-0.024415</td>\n",
       "      <td>0.021406</td>\n",
       "      <td>-0.028677</td>\n",
       "      <td>-0.009823</td>\n",
       "      <td>0.003675</td>\n",
       "      <td>-0.065339</td>\n",
       "      <td>0.021545</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.021373</td>\n",
       "      <td>-0.032757</td>\n",
       "      <td>-0.051921</td>\n",
       "      <td>0.041006</td>\n",
       "      <td>-0.032012</td>\n",
       "      <td>-0.021292</td>\n",
       "      <td>-0.011248</td>\n",
       "      <td>-0.119632</td>\n",
       "      <td>-0.100451</td>\n",
       "      <td>0.084332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241858</th>\n",
       "      <td>-0.081250</td>\n",
       "      <td>0.072284</td>\n",
       "      <td>-0.007992</td>\n",
       "      <td>0.058251</td>\n",
       "      <td>0.044123</td>\n",
       "      <td>0.037043</td>\n",
       "      <td>0.027959</td>\n",
       "      <td>-0.051693</td>\n",
       "      <td>-0.055771</td>\n",
       "      <td>-0.011383</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050207</td>\n",
       "      <td>-0.028469</td>\n",
       "      <td>-0.042262</td>\n",
       "      <td>0.019616</td>\n",
       "      <td>-0.062088</td>\n",
       "      <td>0.051815</td>\n",
       "      <td>0.021332</td>\n",
       "      <td>0.111828</td>\n",
       "      <td>-0.120888</td>\n",
       "      <td>0.056730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241859</th>\n",
       "      <td>-0.059075</td>\n",
       "      <td>0.056959</td>\n",
       "      <td>0.006054</td>\n",
       "      <td>-0.039024</td>\n",
       "      <td>-0.013510</td>\n",
       "      <td>0.021395</td>\n",
       "      <td>0.021009</td>\n",
       "      <td>0.030476</td>\n",
       "      <td>-0.099459</td>\n",
       "      <td>-0.080867</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011553</td>\n",
       "      <td>-0.000094</td>\n",
       "      <td>-0.003660</td>\n",
       "      <td>-0.014688</td>\n",
       "      <td>-0.054684</td>\n",
       "      <td>0.072146</td>\n",
       "      <td>0.047209</td>\n",
       "      <td>0.017928</td>\n",
       "      <td>-0.033454</td>\n",
       "      <td>0.011537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241860</th>\n",
       "      <td>0.004694</td>\n",
       "      <td>0.103036</td>\n",
       "      <td>-0.019062</td>\n",
       "      <td>0.057161</td>\n",
       "      <td>0.039036</td>\n",
       "      <td>-0.003458</td>\n",
       "      <td>0.018490</td>\n",
       "      <td>0.017507</td>\n",
       "      <td>-0.067588</td>\n",
       "      <td>-0.001714</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018214</td>\n",
       "      <td>-0.043566</td>\n",
       "      <td>-0.057350</td>\n",
       "      <td>-0.054574</td>\n",
       "      <td>-0.042672</td>\n",
       "      <td>0.036860</td>\n",
       "      <td>-0.035714</td>\n",
       "      <td>-0.000581</td>\n",
       "      <td>-0.027656</td>\n",
       "      <td>0.066938</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>241861 rows × 384 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6    \\\n",
       "0      -0.032435 -0.009566  0.021242 -0.027626 -0.051214 -0.077786  0.122303   \n",
       "1      -0.046252  0.053345  0.023569 -0.005799 -0.001641  0.005328  0.016217   \n",
       "2      -0.058179 -0.028349  0.028041 -0.001780 -0.056059 -0.040762  0.035624   \n",
       "3       0.001419 -0.005161  0.001925 -0.017756  0.003331 -0.007262  0.074666   \n",
       "4      -0.044808  0.021749 -0.026192 -0.037803 -0.068915 -0.040948 -0.054282   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "241856 -0.028359  0.060098  0.007660 -0.044228 -0.021465 -0.005880 -0.066491   \n",
       "241857  0.001685  0.009930  0.017188 -0.024415  0.021406 -0.028677 -0.009823   \n",
       "241858 -0.081250  0.072284 -0.007992  0.058251  0.044123  0.037043  0.027959   \n",
       "241859 -0.059075  0.056959  0.006054 -0.039024 -0.013510  0.021395  0.021009   \n",
       "241860  0.004694  0.103036 -0.019062  0.057161  0.039036 -0.003458  0.018490   \n",
       "\n",
       "             7         8         9    ...       374       375       376  \\\n",
       "0       0.058641 -0.061732  0.028055  ... -0.097903 -0.045616  0.089570   \n",
       "1      -0.065180 -0.021635  0.044293  ...  0.040859  0.082676  0.012692   \n",
       "2       0.013983 -0.043915 -0.016364  ... -0.044998 -0.046685  0.003407   \n",
       "3       0.010081  0.028236  0.082903  ...  0.016537 -0.042396 -0.030921   \n",
       "4       0.100183 -0.018990  0.021407  ... -0.094901 -0.043734  0.030118   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "241856  0.068297 -0.028384  0.022716  ...  0.036754 -0.022991  0.036425   \n",
       "241857  0.003675 -0.065339  0.021545  ... -0.021373 -0.032757 -0.051921   \n",
       "241858 -0.051693 -0.055771 -0.011383  ...  0.050207 -0.028469 -0.042262   \n",
       "241859  0.030476 -0.099459 -0.080867  ...  0.011553 -0.000094 -0.003660   \n",
       "241860  0.017507 -0.067588 -0.001714  ... -0.018214 -0.043566 -0.057350   \n",
       "\n",
       "             377       378       379       380       381       382       383  \n",
       "0      -0.043381 -0.045833  0.004853  0.044459  0.030812 -0.050800  0.116877  \n",
       "1       0.078683 -0.095079  0.007839  0.124836 -0.024412 -0.000559  0.004015  \n",
       "2       0.010634  0.013347 -0.002498 -0.083653 -0.098793 -0.052496  0.050222  \n",
       "3       0.020760 -0.070070 -0.002282  0.037053  0.009862  0.059242  0.050374  \n",
       "4       0.018265  0.024000 -0.043978  0.041587  0.014160  0.012837  0.041781  \n",
       "...          ...       ...       ...       ...       ...       ...       ...  \n",
       "241856 -0.062824 -0.098093 -0.005447 -0.070600  0.031582  0.085118 -0.016995  \n",
       "241857  0.041006 -0.032012 -0.021292 -0.011248 -0.119632 -0.100451  0.084332  \n",
       "241858  0.019616 -0.062088  0.051815  0.021332  0.111828 -0.120888  0.056730  \n",
       "241859 -0.014688 -0.054684  0.072146  0.047209  0.017928 -0.033454  0.011537  \n",
       "241860 -0.054574 -0.042672  0.036860 -0.035714 -0.000581 -0.027656  0.066938  \n",
       "\n",
       "[241861 rows x 384 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 임베딩 내용 확인\n",
    "emb = pd.DataFrame(embeddings)\n",
    "emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>text</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>...</th>\n",
       "      <th>374</th>\n",
       "      <th>375</th>\n",
       "      <th>376</th>\n",
       "      <th>377</th>\n",
       "      <th>378</th>\n",
       "      <th>379</th>\n",
       "      <th>380</th>\n",
       "      <th>381</th>\n",
       "      <th>382</th>\n",
       "      <th>383</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A07936821FOVJO6NP4Q8</td>\n",
       "      <td>B0000A0AEM</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Very nice product.  Sharp, clear optics.  Seem...</td>\n",
       "      <td>-0.032435</td>\n",
       "      <td>-0.009566</td>\n",
       "      <td>0.021242</td>\n",
       "      <td>-0.027626</td>\n",
       "      <td>-0.051214</td>\n",
       "      <td>-0.077786</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.097903</td>\n",
       "      <td>-0.045616</td>\n",
       "      <td>0.089570</td>\n",
       "      <td>-0.043381</td>\n",
       "      <td>-0.045833</td>\n",
       "      <td>0.004853</td>\n",
       "      <td>0.044459</td>\n",
       "      <td>0.030812</td>\n",
       "      <td>-0.050800</td>\n",
       "      <td>0.116877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A07936821FOVJO6NP4Q8</td>\n",
       "      <td>B0000AI0N1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Just what it said it would be, well made, wort...</td>\n",
       "      <td>-0.046252</td>\n",
       "      <td>0.053345</td>\n",
       "      <td>0.023569</td>\n",
       "      <td>-0.005799</td>\n",
       "      <td>-0.001641</td>\n",
       "      <td>0.005328</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040859</td>\n",
       "      <td>0.082676</td>\n",
       "      <td>0.012692</td>\n",
       "      <td>0.078683</td>\n",
       "      <td>-0.095079</td>\n",
       "      <td>0.007839</td>\n",
       "      <td>0.124836</td>\n",
       "      <td>-0.024412</td>\n",
       "      <td>-0.000559</td>\n",
       "      <td>0.004015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A07936821FOVJO6NP4Q8</td>\n",
       "      <td>B0002RSPE4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Bought a bunch of these a few years ago for th...</td>\n",
       "      <td>-0.058179</td>\n",
       "      <td>-0.028349</td>\n",
       "      <td>0.028041</td>\n",
       "      <td>-0.001780</td>\n",
       "      <td>-0.056059</td>\n",
       "      <td>-0.040762</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.044998</td>\n",
       "      <td>-0.046685</td>\n",
       "      <td>0.003407</td>\n",
       "      <td>0.010634</td>\n",
       "      <td>0.013347</td>\n",
       "      <td>-0.002498</td>\n",
       "      <td>-0.083653</td>\n",
       "      <td>-0.098793</td>\n",
       "      <td>-0.052496</td>\n",
       "      <td>0.050222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A07936821FOVJO6NP4Q8</td>\n",
       "      <td>B000CRFOMK</td>\n",
       "      <td>5.0</td>\n",
       "      <td>At first glance these seem silly.  Who needs a...</td>\n",
       "      <td>0.001419</td>\n",
       "      <td>-0.005161</td>\n",
       "      <td>0.001925</td>\n",
       "      <td>-0.017756</td>\n",
       "      <td>0.003331</td>\n",
       "      <td>-0.007262</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016537</td>\n",
       "      <td>-0.042396</td>\n",
       "      <td>-0.030921</td>\n",
       "      <td>0.020760</td>\n",
       "      <td>-0.070070</td>\n",
       "      <td>-0.002282</td>\n",
       "      <td>0.037053</td>\n",
       "      <td>0.009862</td>\n",
       "      <td>0.059242</td>\n",
       "      <td>0.050374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A07936821FOVJO6NP4Q8</td>\n",
       "      <td>B000ID7QNI</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Very sturdy well made wall mount.</td>\n",
       "      <td>-0.044808</td>\n",
       "      <td>0.021749</td>\n",
       "      <td>-0.026192</td>\n",
       "      <td>-0.037803</td>\n",
       "      <td>-0.068915</td>\n",
       "      <td>-0.040948</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.094901</td>\n",
       "      <td>-0.043734</td>\n",
       "      <td>0.030118</td>\n",
       "      <td>0.018265</td>\n",
       "      <td>0.024000</td>\n",
       "      <td>-0.043978</td>\n",
       "      <td>0.041587</td>\n",
       "      <td>0.014160</td>\n",
       "      <td>0.012837</td>\n",
       "      <td>0.041781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241856</th>\n",
       "      <td>AZZTOUKVTUMVM</td>\n",
       "      <td>B003FVJYF8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>It's 50' of coax. Seems to be well constructed...</td>\n",
       "      <td>-0.028359</td>\n",
       "      <td>0.060098</td>\n",
       "      <td>0.007660</td>\n",
       "      <td>-0.044228</td>\n",
       "      <td>-0.021465</td>\n",
       "      <td>-0.005880</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036754</td>\n",
       "      <td>-0.022991</td>\n",
       "      <td>0.036425</td>\n",
       "      <td>-0.062824</td>\n",
       "      <td>-0.098093</td>\n",
       "      <td>-0.005447</td>\n",
       "      <td>-0.070600</td>\n",
       "      <td>0.031582</td>\n",
       "      <td>0.085118</td>\n",
       "      <td>-0.016995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241857</th>\n",
       "      <td>AZZTOUKVTUMVM</td>\n",
       "      <td>B00BUL4NLU</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Bought this at the end of September 2016 for u...</td>\n",
       "      <td>0.001685</td>\n",
       "      <td>0.009930</td>\n",
       "      <td>0.017188</td>\n",
       "      <td>-0.024415</td>\n",
       "      <td>0.021406</td>\n",
       "      <td>-0.028677</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.021373</td>\n",
       "      <td>-0.032757</td>\n",
       "      <td>-0.051921</td>\n",
       "      <td>0.041006</td>\n",
       "      <td>-0.032012</td>\n",
       "      <td>-0.021292</td>\n",
       "      <td>-0.011248</td>\n",
       "      <td>-0.119632</td>\n",
       "      <td>-0.100451</td>\n",
       "      <td>0.084332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241858</th>\n",
       "      <td>AZZTOUKVTUMVM</td>\n",
       "      <td>B00HWT8I24</td>\n",
       "      <td>3.0</td>\n",
       "      <td>It was a bit challenging to get this to synch ...</td>\n",
       "      <td>-0.081250</td>\n",
       "      <td>0.072284</td>\n",
       "      <td>-0.007992</td>\n",
       "      <td>0.058251</td>\n",
       "      <td>0.044123</td>\n",
       "      <td>0.037043</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050207</td>\n",
       "      <td>-0.028469</td>\n",
       "      <td>-0.042262</td>\n",
       "      <td>0.019616</td>\n",
       "      <td>-0.062088</td>\n",
       "      <td>0.051815</td>\n",
       "      <td>0.021332</td>\n",
       "      <td>0.111828</td>\n",
       "      <td>-0.120888</td>\n",
       "      <td>0.056730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241859</th>\n",
       "      <td>AZZTOUKVTUMVM</td>\n",
       "      <td>B00IX9ZDKC</td>\n",
       "      <td>5.0</td>\n",
       "      <td>What can I say. It worked great and my compute...</td>\n",
       "      <td>-0.059075</td>\n",
       "      <td>0.056959</td>\n",
       "      <td>0.006054</td>\n",
       "      <td>-0.039024</td>\n",
       "      <td>-0.013510</td>\n",
       "      <td>0.021395</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011553</td>\n",
       "      <td>-0.000094</td>\n",
       "      <td>-0.003660</td>\n",
       "      <td>-0.014688</td>\n",
       "      <td>-0.054684</td>\n",
       "      <td>0.072146</td>\n",
       "      <td>0.047209</td>\n",
       "      <td>0.017928</td>\n",
       "      <td>-0.033454</td>\n",
       "      <td>0.011537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241860</th>\n",
       "      <td>AZZTOUKVTUMVM</td>\n",
       "      <td>B00TQD2BYK</td>\n",
       "      <td>4.0</td>\n",
       "      <td>It was a bit temperamental to get it set-up, b...</td>\n",
       "      <td>0.004694</td>\n",
       "      <td>0.103036</td>\n",
       "      <td>-0.019062</td>\n",
       "      <td>0.057161</td>\n",
       "      <td>0.039036</td>\n",
       "      <td>-0.003458</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018214</td>\n",
       "      <td>-0.043566</td>\n",
       "      <td>-0.057350</td>\n",
       "      <td>-0.054574</td>\n",
       "      <td>-0.042672</td>\n",
       "      <td>0.036860</td>\n",
       "      <td>-0.035714</td>\n",
       "      <td>-0.000581</td>\n",
       "      <td>-0.027656</td>\n",
       "      <td>0.066938</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>241861 rows × 388 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     item_id     user_id  rating  \\\n",
       "0       A07936821FOVJO6NP4Q8  B0000A0AEM     3.0   \n",
       "1       A07936821FOVJO6NP4Q8  B0000AI0N1     5.0   \n",
       "2       A07936821FOVJO6NP4Q8  B0002RSPE4     5.0   \n",
       "3       A07936821FOVJO6NP4Q8  B000CRFOMK     5.0   \n",
       "4       A07936821FOVJO6NP4Q8  B000ID7QNI     5.0   \n",
       "...                      ...         ...     ...   \n",
       "241856         AZZTOUKVTUMVM  B003FVJYF8     4.0   \n",
       "241857         AZZTOUKVTUMVM  B00BUL4NLU     2.0   \n",
       "241858         AZZTOUKVTUMVM  B00HWT8I24     3.0   \n",
       "241859         AZZTOUKVTUMVM  B00IX9ZDKC     5.0   \n",
       "241860         AZZTOUKVTUMVM  B00TQD2BYK     4.0   \n",
       "\n",
       "                                                     text         0         1  \\\n",
       "0       Very nice product.  Sharp, clear optics.  Seem... -0.032435 -0.009566   \n",
       "1       Just what it said it would be, well made, wort... -0.046252  0.053345   \n",
       "2       Bought a bunch of these a few years ago for th... -0.058179 -0.028349   \n",
       "3       At first glance these seem silly.  Who needs a...  0.001419 -0.005161   \n",
       "4                       Very sturdy well made wall mount. -0.044808  0.021749   \n",
       "...                                                   ...       ...       ...   \n",
       "241856  It's 50' of coax. Seems to be well constructed... -0.028359  0.060098   \n",
       "241857  Bought this at the end of September 2016 for u...  0.001685  0.009930   \n",
       "241858  It was a bit challenging to get this to synch ... -0.081250  0.072284   \n",
       "241859  What can I say. It worked great and my compute... -0.059075  0.056959   \n",
       "241860  It was a bit temperamental to get it set-up, b...  0.004694  0.103036   \n",
       "\n",
       "               2         3         4         5  ...       374       375  \\\n",
       "0       0.021242 -0.027626 -0.051214 -0.077786  ... -0.097903 -0.045616   \n",
       "1       0.023569 -0.005799 -0.001641  0.005328  ...  0.040859  0.082676   \n",
       "2       0.028041 -0.001780 -0.056059 -0.040762  ... -0.044998 -0.046685   \n",
       "3       0.001925 -0.017756  0.003331 -0.007262  ...  0.016537 -0.042396   \n",
       "4      -0.026192 -0.037803 -0.068915 -0.040948  ... -0.094901 -0.043734   \n",
       "...          ...       ...       ...       ...  ...       ...       ...   \n",
       "241856  0.007660 -0.044228 -0.021465 -0.005880  ...  0.036754 -0.022991   \n",
       "241857  0.017188 -0.024415  0.021406 -0.028677  ... -0.021373 -0.032757   \n",
       "241858 -0.007992  0.058251  0.044123  0.037043  ...  0.050207 -0.028469   \n",
       "241859  0.006054 -0.039024 -0.013510  0.021395  ...  0.011553 -0.000094   \n",
       "241860 -0.019062  0.057161  0.039036 -0.003458  ... -0.018214 -0.043566   \n",
       "\n",
       "             376       377       378       379       380       381       382  \\\n",
       "0       0.089570 -0.043381 -0.045833  0.004853  0.044459  0.030812 -0.050800   \n",
       "1       0.012692  0.078683 -0.095079  0.007839  0.124836 -0.024412 -0.000559   \n",
       "2       0.003407  0.010634  0.013347 -0.002498 -0.083653 -0.098793 -0.052496   \n",
       "3      -0.030921  0.020760 -0.070070 -0.002282  0.037053  0.009862  0.059242   \n",
       "4       0.030118  0.018265  0.024000 -0.043978  0.041587  0.014160  0.012837   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "241856  0.036425 -0.062824 -0.098093 -0.005447 -0.070600  0.031582  0.085118   \n",
       "241857 -0.051921  0.041006 -0.032012 -0.021292 -0.011248 -0.119632 -0.100451   \n",
       "241858 -0.042262  0.019616 -0.062088  0.051815  0.021332  0.111828 -0.120888   \n",
       "241859 -0.003660 -0.014688 -0.054684  0.072146  0.047209  0.017928 -0.033454   \n",
       "241860 -0.057350 -0.054574 -0.042672  0.036860 -0.035714 -0.000581 -0.027656   \n",
       "\n",
       "             383  \n",
       "0       0.116877  \n",
       "1       0.004015  \n",
       "2       0.050222  \n",
       "3       0.050374  \n",
       "4       0.041781  \n",
       "...          ...  \n",
       "241856 -0.016995  \n",
       "241857  0.084332  \n",
       "241858  0.056730  \n",
       "241859  0.011537  \n",
       "241860  0.066938  \n",
       "\n",
       "[241861 rows x 388 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 본 데이터와 합병\n",
    "data = pd.concat([data, emb], axis=1)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 준비 (인코딩)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding dictionaries\n",
    "def create_encoding_dict(feature, start_point):\n",
    "    feature_dict = {}\n",
    "    for value in set(feature):\n",
    "        feature_dict[value] = start_point + len(feature_dict)\n",
    "    return feature_dict, start_point + len(feature_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용자, 아이템, 직업, 성별 인코딩\n",
    "user_dict, start_point = create_encoding_dict(data['user_id'], 0)\n",
    "item_dict, start_point = create_encoding_dict(data['item_id'], start_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Users: 17355\n",
      "Number of Items: 17244\n",
      "전체 특성 수: 34983\n"
     ]
    }
   ],
   "source": [
    "# 텍스트 임베딩\n",
    "text_index = start_point\n",
    "start_point += 384\n",
    "\n",
    "# 전체 특성 수 계산\n",
    "num_x = start_point\n",
    "\n",
    "# 각 특성의 개수 출력 (선택적)\n",
    "print(f\"Number of Users: {len(user_dict)}\")\n",
    "print(f\"Number of Items: {len(item_dict)}\")\n",
    "print(f\"전체 특성 수: {num_x}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34983"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data.copy()\n",
    "y = data['user_id']\n",
    "ratings_train, ratings_test = train_test_split(x, test_size=0.25, stratify=y, random_state=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train set 평점의 평균값 -> 타겟 변수에서 빼서 평균 평점에 대한 보정 진행\n",
    "w0 = np.mean(ratings_train['rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data(input, bias, user_dict, item_dict, embeddings_start_idx):\n",
    "    data = []\n",
    "    target = []\n",
    "\n",
    "    for i in range(len(input)):\n",
    "        ea_case = input.iloc[i]\n",
    "        x_index = []\n",
    "        x_value = []\n",
    "\n",
    "        # user id encoding\n",
    "        x_index.append(user_dict[ea_case['user_id']])\n",
    "        x_value.append(1.)\n",
    "\n",
    "        # item id encoding\n",
    "        x_index.append(item_dict[ea_case['item_id']])\n",
    "        x_value.append(1.)\n",
    "        \n",
    "        # review encoding\n",
    "        review_embed = ea_case[-384:]        # 해당 리뷰의 임베딩\n",
    "        for j in range(384):\n",
    "            x_index.append(embeddings_start_idx+j)\n",
    "            x_value.append(review_embed[j])\n",
    "\n",
    "        # target encoding\n",
    "        data.append([x_index, x_value])\n",
    "        target.append(ea_case['rating']-bias)\n",
    "\n",
    "        # 진행 상황 출력\n",
    "        if (i % 30000) == 0:\n",
    "            print('Encoding ', i, 'cases...')\n",
    "    \n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding Train Set\n",
      "Encoding  0 cases...\n",
      "Encoding  10000 cases...\n",
      "Encoding  20000 cases...\n",
      "Encoding  30000 cases...\n",
      "Encoding  40000 cases...\n",
      "Encoding  50000 cases...\n",
      "Encoding  60000 cases...\n",
      "Encoding  70000 cases...\n",
      "Encoding  80000 cases...\n",
      "Encoding  90000 cases...\n",
      "Encoding  100000 cases...\n",
      "Encoding  110000 cases...\n",
      "Encoding  120000 cases...\n",
      "Encoding  130000 cases...\n",
      "Encoding  140000 cases...\n",
      "Encoding  150000 cases...\n",
      "Encoding  160000 cases...\n",
      "Encoding  170000 cases...\n",
      "Encoding  180000 cases...\n",
      "Encoding Test Set\n",
      "Encoding  0 cases...\n",
      "Encoding  10000 cases...\n",
      "Encoding  20000 cases...\n",
      "Encoding  30000 cases...\n",
      "Encoding  40000 cases...\n",
      "Encoding  50000 cases...\n",
      "Encoding  60000 cases...\n"
     ]
    }
   ],
   "source": [
    "print('Encoding Train Set')\n",
    "train_data, train_target = encode_data(ratings_train, w0, user_dict, item_dict, text_index)\n",
    "print('Encoding Test Set')\n",
    "test_data, test_target = encode_data(ratings_test, w0, user_dict, item_dict, text_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSE(y_true, y_pred):\n",
    "    return np.sqrt(np.mean((np.array(y_true) - np.array(y_pred))**2))\n",
    "\n",
    "class FM():\n",
    "    def __init__(self, N, K, train_x, train_y, test_x, test_y, alpha, beta, iterations=100, tolerance=0.005, l2_reg=True, verbose=True): # 초기화\n",
    "        self.K = K                          # Number of latent factors\n",
    "        self.N = N                          # Number of x (variables)\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.iterations = iterations\n",
    "        self.l2_reg = l2_reg\n",
    "        self.tolerance = tolerance\n",
    "        self.verbose = verbose\n",
    "\n",
    "        # w와 v 초기화\n",
    "        self.w = np.random.normal(scale=1./self.N, size=(self.N)) # 사이즈는 변수의 수만큼. 변수마다 bias 하나\n",
    "        self.v = np.random.normal(scale=1./self.K, size=(self.N, self.K)) # 변수의 수 * K\n",
    "\n",
    "        # Train/Test 분리\n",
    "        self.train_x = train_x\n",
    "        self.test_x = test_x\n",
    "        self.train_y = train_y\n",
    "        self.test_y = test_y\n",
    "\n",
    "    def test(self):                                     # Training 하면서 RMSE 계산 \n",
    "        # SGD를 iterations 숫자만큼 수행\n",
    "        best_RMSE = float('inf') # stop 위해\n",
    "        best_iteration = 0\n",
    "        training_process = []\n",
    "        for i in range(self.iterations): # 600번\n",
    "            rmse1 = self.sgd(self.train_x, self.train_y)        # SGD & Train RMSE 계산\n",
    "            rmse2 = self.test_rmse(self.test_x, self.test_y)    # Test RMSE 계산     \n",
    "            training_process.append((i, rmse1, rmse2))\n",
    "            if self.verbose:\n",
    "                if (i+1) % 10 == 0:\n",
    "                    print(\"Iteration: %d ; Train RMSE = %.6f ; Test RMSE = %.6f\" % (i+1, rmse1, rmse2))\n",
    "            if best_RMSE > rmse2:                       # New best record\n",
    "                best_RMSE = rmse2\n",
    "                best_iteration = i\n",
    "            elif (rmse2 - best_RMSE) > self.tolerance:  # RMSE is increasing over tolerance\n",
    "                break\n",
    "        print(best_iteration, best_RMSE)\n",
    "        return training_process\n",
    "        \n",
    "    # w, v 업데이트를 위한 Stochastic gradient descent \n",
    "    def sgd(self, x_data, y_data):\n",
    "        y_pred = []\n",
    "        for data, y in zip(x_data, y_data): # 100,000번. x_data, y_data가 100,000개\n",
    "            x_idx = data[0] # 데이터의 첫번째 (x_index, x_value)에 대한 인덱스 받아옴\n",
    "            x_0 = np.array(data[1])     # xi axis=0 [1, 2, 3] (1차원)\n",
    "            x_1 = x_0.reshape(-1, 1)    # xi axis=1 [[1], [2], [3]] (2차원: V matrix와 계산 위해서)\n",
    "    \n",
    "            # biases\n",
    "            bias_score = np.sum(self.w[x_idx] * x_0) # 여기선 x_0를 1차원으로 사용. w matrix는 1차원이기 때문\n",
    "    \n",
    "            # score 계산\n",
    "            vx = self.v[x_idx] * (x_1)          # v matrix * x (브로드캐스팅)\n",
    "            sum_vx = np.sum(vx, axis=0)         # sigma(vx): 칼럼으로 쭉 더한 것 (element K개 (=350개))\n",
    "            sum_vx_2 = np.sum(vx * vx, axis=0)  # ( v matrix * x )의 제곱: element 350개\n",
    "            latent_score = 0.5 * np.sum(np.square(sum_vx) - sum_vx_2)\n",
    "\n",
    "            # 예측값 계산\n",
    "            y_hat = bias_score + latent_score # bias까지 더하면 최종 예측값 (전체 평균은 전에 뺐기 때문에 따로 또 빼주지 않음)\n",
    "            y_pred.append(y_hat) # y_pred 75,000개 (아까 train,test 분리함)\n",
    "            error = y - y_hat # 에러 구했으니까 아래에서 업데이트 가능\n",
    "            # w, v 업데이트 (week 7 수업자료에 있는 update rule)\n",
    "            if self.l2_reg:     # regularization이 있는 경우\n",
    "                self.w[x_idx] += error * self.alpha * (x_0 - self.beta * self.w[x_idx])\n",
    "                self.v[x_idx] += error * self.alpha * ((x_1) * sum(vx) - (vx * x_1) - self.beta * self.v[x_idx])\n",
    "            else:               # regularization이 없는 경우\n",
    "                self.w[x_idx] += error * self.alpha * x_0\n",
    "                self.v[x_idx] += error * self.alpha * ((x_1) * sum(vx) - (vx * x_1))\n",
    "        return RMSE(y_data, y_pred) \n",
    "\n",
    "    def test_rmse(self, x_data, y_data): # test set에 대한 RMSE\n",
    "        y_pred = []\n",
    "        for data , y in zip(x_data, y_data):\n",
    "            y_hat = self.predict(data[0], data[1])\n",
    "            y_pred.append(y_hat)\n",
    "        return RMSE(y_data, y_pred)\n",
    "\n",
    "    def predict(self, idx, x):\n",
    "        x_0 = np.array(x)\n",
    "        x_1 = x_0.reshape(-1, 1)\n",
    "\n",
    "        # biases\n",
    "        bias_score = np.sum(self.w[idx] * x_0)\n",
    "\n",
    "        # score 계산\n",
    "        vx = self.v[idx] * (x_1)\n",
    "        sum_vx = np.sum(vx, axis=0)\n",
    "        sum_vx_2 = np.sum(vx * vx, axis=0)\n",
    "        latent_score = 0.5 * np.sum(np.square(sum_vx) - sum_vx_2)\n",
    "\n",
    "        # 예측값 계산\n",
    "        y_hat = bias_score + latent_score\n",
    "        return y_hat\n",
    "\n",
    "    def predict_one(self, user_id, movie_id):\n",
    "        x_idx = np.array([user_dict[user_id], item_dict[movie_id]])\n",
    "        x_data = np.array([1, 1])\n",
    "        return self.predict(x_idx, x_data) + w0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 10 ; Train RMSE = 0.916461 ; Test RMSE = 0.945540\n",
      "Iteration: 20 ; Train RMSE = 0.830261 ; Test RMSE = 0.921580\n",
      "24 0.9198331413464679\n"
     ]
    }
   ],
   "source": [
    "K = 220\n",
    "fm1 = FM(num_x, K, train_data, train_target, test_data, test_target, alpha=0.0014, beta=0.003,  \n",
    "         iterations=400, tolerance=0.0005, l2_reg=True, verbose=True)\n",
    "\n",
    "result = fm1.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/ryu/thesis/new_amazon/state8/FM_model8.pkl', 'wb') as f:\n",
    "    pickle.dump(fm1, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ryuvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
